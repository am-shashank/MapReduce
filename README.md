# MapReduce
MapReduce framework will consist of two types of nodes: a number of workers and a single master.  The workers are in charge of running the map and reduce functions, and of storing the data that the MapReduce framework is working on; the master coordinates the workers and provides a user interface.  Each worker will have a storage directory in which it keeps its local share of the data. For instance, a   worker might have a storage directory called ~/store, with a subdirectory ~/store/webpages in which it has collected some web pages from crawling, and a subdirectory ~/store/index in which it   creates an inverted index based on the text from these web pages. Keep in mind that there will usually be   many workers, and that the data will be distributed among them; for instance, you might have a system   with 10 workers and 100,000 web pages total, but each worker might only store 10,000 of them.  The master will have a status page on which it displays the list of workers that are currently online, as   well as some information about each (e.g., the worker's IP address, and what the worker is doing). To   keep this list up to date, the workers will periodically send some information about themselves to the   master. The status page will also have an input form that allows the administrator to specify a MapReduce   job to run, as well as some parameters (such as a subdirectory of the storage directory to read data from, a   subdirectory to write the output to, etc.). When the administrator submits this form, the master forwards   this information to each of the workers, which then begin processing the data.  The master and the workers will all be implemented as servlets. The actual MapReduce jobs will simply be classes that implement a special interface (which contains a map and a reduce function). In a   MapReduce framework like Hadoop, these classes would be sent from the master to the workers; We will assume that these classes are already in the classpath on each worker.
